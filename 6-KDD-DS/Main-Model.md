# 主要模型

## 目录

* [说明](#说明)
* [机器学习算法](#机器学习算法)
  * [监督学习](#监督学习)
  * [非监督学习](#非监督学习)
  * [强化学习](#强化学习)
* [主要算法](#主要算法)
  * [决策树](#决策树)
  * [随机森林](#随机森林)
  * [Adaboost](#Adaboost)
  * [GBDT](#GBDT)
  * [逻辑回归](#逻辑回归)
  * [支持向量机](#支持向量机)
  * [朴素贝叶斯](#朴素贝叶斯)
* [其他算法](#其他算法)
  * [线性回归](#线性回归)
  * [k最邻近算法](#k最近邻算法)
  * [人工神经网络](#人工神经网络)
  * [K-Means](#K-Means)
* [其他库](#其他库)
  * [xgboost](#xgboost)
  * [lightgbm](#lightgbm)
* [模型对比](#模型对比)
  * [scikit-cheet-sheat](#scikit-cheet-sheat)
  * [主要算法对比](#主要算法对比)
## 说明

这里主要介绍模型的基本原理、公式推导和对比

## 线性回归

#### 介绍

由于预测建模主要关注最小化模型的误差，或者以可解释性为代价来做出最准确的预测。 我们会从许多不同领域借用、重用和盗用算法，其中涉及一些统计学知识。

线性回归用一个等式表示，通过找到输入变量的特定权重（B），来描述输入变量（x）与输出变量（y）之间的线性关系。

#### 使用范围
#### 特点
###### 优点

- 实现简单，计算简单；

###### 缺点

- 不能拟合非线性数据.

## 逻辑回归

- 又称 logistic回归、

#### 介绍

逻辑回归模型是一个二分类模型，它选取不同的特征与权重来对样本进行概率分类，用一各log函数计算样本属于某一类的概率。即一个样本会有一定的概率属于一个类，会有一定的概率属于另一类，概率大的类即为样本所属类。
逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。

简单粗暴的说：逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应为二类时的特殊情况，也就是说，当逻辑回归扩展为多类别的时候，就是最大熵模型。

#### 使用范围
#### 特点
###### 优点

- 实现简单，应用成熟，基本所有的机器学习库都有
- 支持增量学习，可以在线更新
- 可以输出概率，结果易于解释
- 分类时计算量非常小，速度很快，存储资源低；

###### 缺点

- 全局最优、容易欠拟合
- 当特征空间太大时表现较弱。当特征空间很大时，逻辑回归的性能不是很好；
- 特征离散化
- 不能很好地处理大量多类特征或变量；
- 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；难以捕捉非线性特征



## 决策树

#### 介绍

- 决策树是一种基本的分类与回归方法，其模型就是用一棵树来表示我们的整个决策过程。这棵树可以是二叉树（比如CART 只能是二叉树），也可以是多叉树（比如 ID3、C4.5 可以是多叉树或二叉树）。根节点包含整个样本集，每个叶节都对应一个决策结果（注意，不同的叶节点可能对应同一个决策结果），每一个内部节点都对应一次决策过程或者说是一次属性测试。从根节点到每个叶节点的路径对应一个判定测试序列。
- 决策树的生成就是不断的选择最优的特征对训练集进行划分，是一个递归的过程。递归返回的条件有三种：
    - 当前节点包含的样本属于同一类别，无需划分；
    - 当前属性集为空，或所有样本在属性集上取值相同，无法划分；
    - 当前节点包含样本集合为空，无法划分。
- ID3、C4.5、CART这三个是非常著名的决策树算法。简单粗暴来说:
  - ID3 使用信息增益作为选择特征的准则；
  - C4.5 使用信息增益比作为选择特征的准则；
  - CART 使用 Gini 指数作为选择特征的准则。
- 熵(entropy)
  - 在信息论与概率论中，熵用于表示随机变量不确定性的度量。

###### ID3
###### C4.5
###### CART

#### 使用范围
#### 特点

###### 优点

- 计算简单，易于理解，可解释性强；
- 比较适合处理有缺失属性的样本；对数据预处理要求不高。能够处理不相关的特征；
- 在相对短的时间内能够对大型数据源做出可行且效果良好的结果。

###### 缺点

- 讲究局部最优，容易过拟合。容易发生过拟合（随机森林可以很大程度上减少过拟合）；
- 容易忽略数据之间的相关性。忽略了数据之间的相关性；
- 对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征（只要是使用了信息增益，都有这个缺点，如RF）。

## 随机森林

#### 介绍
#### 使用范围
#### 特点
###### 优点
###### 缺点

## Adaboost

#### 介绍
#### 使用范围
#### 特点

###### 优点

- 精度高，不容易过拟合
- 不用做特征选择
- 可以使用各种方法构建子分类器，Adaboost算法提供的是框架。
- 当使用简单分类器时，计算出的结果是可以理解的，并且弱分类器的构造极其简单。
- 不易发生overfitting。
Adaboost作为分类器时，分类精度很高
在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。
作为简单的二元分类器时，构造简单，结果可理解。
不容易发生过拟合

###### 缺点

- 对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。

## GBDT

- 又称 Gradient Boost、

#### 介绍

- DT：回归树 Regression Decision Tree

GBDT 中的树全部都是回归树，核心就是累加所有树的结果作为最终结果。只有回归树的结果累加起来才是有意义的，分类的结果累加是没有意义的。

GBDT 调整之后可以用于分类问题，但是内部还是回归树。

这部分和决策树中的是一样的，无非就是特征选择。回归树用的是最小化均方误差，分类树是用的是最小化基尼指数（CART）

- 梯度迭代 Gradient Boosting
首先 Boosting 是一种集成方法。通过对弱分类器的组合得到强分类器，他是串行的，几个弱分类器之间是依次训练的。GBDT 的核心就在于，每一颗树学习的是之前所有树结论和的残差。

Gradient 体现在：无论前面一颗树的 cost function 是什么，是均方差还是均差，只要它以误差作为衡量标准，那么残差向量都是它的全局最优方向，这就是 Gradient。

- Shrinkage（缩减）是 GBDT 算法的一个重要演进分支，目前大部分的源码都是基于这个版本的。

核心思想在于：Shrinkage 认为每次走一小步来逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易防止过拟合。

也就是说，它不信任每次学习到的残差，它认为每棵树只学习到了真理的一小部分，累加的时候只累加一小部分，通过多学习几棵树来弥补不足。

具体的做法就是：仍然以残差作为学习目标，但是对于残差学习出来的结果，只累加一小部分（step* 残差）逐步逼近目标，step 一般都比较小 0.01-0.001, 导致各个树的残差是渐变而不是陡变的。

本质上，Shrinkage 为每一颗树设置了一个 weight，累加时要乘以这个 weight，但和 Gradient 没有关系。

这个 weight 就是 step。跟 AdaBoost 一样，Shrinkage 能减少过拟合也是经验证明的，目前还没有理论证明。

#### 使用范围

GBDT 可以适用于回归问题（线性和非线性）；
GBDT 也可用于二分类问题（设定阈值，大于为正，否则为负）和多分类问题。


#### 特点
###### 优点
###### 缺点

## 支持向量机

- 又称 SVM、

#### 介绍

支持向量机是一个二分类算法，它可以在N维空间找到一个(N-1)维的超平面，这个超平面可以将这些点分为两类。也就是说，平面内如果存在线性可分的两类点，SVM可以找到一条最优的直线将这些点分开。SVM应用范围很广。

- SVM：
  - 线性可分支持向量机。当训练数据线性可分时，通过硬间隔最大化，学习到的一个线性分类器；
  - 线性支持向量机。当训练数据近似线性可分时，通过软间隔最大化，学习到的一个线性分类器；
  - 非线性支持向量机。当训练数据线性不可分，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

- 对于核的选择也是有技巧的（libsvm中自带了四种核函数：线性核、多项式核、RBF以及sigmoid核）：
  - 第一，如果样本数量小于特征数，那么就没必要选择非线性核，简单的使用线性核就可以了；
  - 第二，如果样本数量大于特征数目，这时可以使用非线性核，将样本映射到更高维度，一般可以得到更好的结果；
  - 第三，如果样本数目和特征数目相等，该情况可以使用非线性核，原理和第二种一样。
- 对于第一种情况，也可以先对数据进行降维，然后使用非线性核，这也是一种方法。

#### 使用范围
#### 特点
###### 优点

- 使用核函数避开了高纬空间的复杂性；可以解决高纬数据集，适合文本/图像分类
- 能处理非线性可分数据。能够处理非线性特征的相互作用；
- 高泛化能力
- 无需依赖整个数据；在小训练集上往往得到比较好的结果；


###### 缺点

- 计算量大。当观测样本很多时，效率并不是很高；时空开销比较大，训练时间长；
- 需要调参数、寻找核函数。对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数；核函数的选取比较难，主要靠经验。
- 对缺失数据敏感


## 朴素贝叶斯

#### 介绍

朴素贝叶斯分类器基于贝叶斯理论及其假设（即特征之间是独立的，是不相互影响的）
P(A|B) 是后验概率， P(B|A) 是似然，P(A)为先验概率，P(B) 为我们要预测的值。

具体应用有：垃圾邮件检测、文章分类、情感分类、人脸识别等。
朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用，用mRMR中R来讲，就是特征冗余。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。

#### 使用范围
#### 特点
###### 优点

- 算法简单，收敛速度快
- 在小数据集上表现较好，多用于文本分类。对缺失数据不太敏感，算法也比较简单，常用于文本分类。
- 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。
- 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练；支持增量学习

###### 缺点

- 需要计算先验概率；
- 分类决策存在错误率；
- 对输入数据的表达形式很敏感。

## xgboost

#### 介绍

XGBoost = Extreme + Gradient Boosting（极端 + 梯度 提升） = Gradient Boosting Decision Tree（梯度提升决策树） = Gradient + Boosting Decision Tree = Boosting + CART

#### 使用范围
#### 特点
###### 优点
###### 缺点

https://www.zhihu.com/question/41354392

## lightgbm

#### 介绍
#### 使用范围
#### 特点
###### 优点
###### 缺点



## k最近邻算法

- 又称 KNN、

#### 介绍
#### 使用范围
#### 特点
###### 优点

- 思想简单，可用以分类也可回归。理论成熟，思想简单，既可以用来做分类也可以用来做回归；
- 可以用以非线性分类
- 训练时间复杂度为O(n)；
- 对数据没有假设，准确度高，对outlier不敏感；

###### 缺点

- 计算量大（体现在距离计算上）；
- 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差；
- 需要大量内存；

## K-Means

- 又称 k均值、K-Means聚类

#### 介绍
#### 使用范围
#### 特点
###### 优点

- 原理简单，容易实现
- 当簇之间区分效果明显时聚类效果较好
- 对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k<<n。这个算法通常局部收敛。
- 算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。
###### 缺点

- 对初始质心及K的选择较敏感
- 对异常点敏感
- 对数据类型要求较高，适合数值型数据；
- 可能收敛到局部最小值，在大规模数据上收敛较慢
- K值比较难以选取；
- 对初值的簇心值敏感，对于不同的初始值，可能会导致不同的聚类结果；
- 不适合于发现非凸面形状的簇，或者大小差别很大的簇。
- 对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。

## 聚类

#### 介绍
聚类算法就是将一堆数据进行处理，根据它们的相似性对数据进行聚类。聚类算法有很多种，具体如下：中心聚类、关联聚类、密度聚类、概率聚类、降维、神经网络/深度学习。


#### 使用范围
#### 特点
###### 优点
###### 缺点

## 关联规则学习

## 人工神经网络

#### 介绍

#### 使用范围
#### 特点
###### 优点

- 分类的准确度高；
- 并行分布处理能力强,分布存储及学习能力强，
- 对噪声神经有较强的鲁棒性和容错能力，能充分逼近复杂的非线性关系；
- 具备联想记忆的功能。

###### 缺点

- 神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值；
- 不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度；
- 学习时间过长,甚至可能达不到学习的目的。

## 集成学习

#### 介绍

集成学习就是将很多分类器集成在一起，每个分类器有不同的权重，将这些分类器的分类结果合并在一起，作为最终的分类结果。最初集成方法为贝叶斯决策，现在多采用error-correcting output coding, bagging, and boosting等方法进行集成。
那么为什集成分类器要比单个分类器效果好呢？

1.偏差均匀化：如果你将民主党与共和党的投票数算一下均值，可定会得到你原先没有发现的结果，集成学习与这个也类似，它可以学到其它任何一种方式都学不到的东西。

2.减少方差：总体的结果要比单一模型的结果好，因为其从多个角度考虑问题。类似于股票市场，综合考虑多只股票可以要比只考虑一只股票好，这就是为什么多数据比少数据效果好原因，因为其考虑的因素更多。

3.不容易过拟合。如果的一个模型不过拟合，那么综合考虑多种因素的多模型就更不容易过拟合了。

#### 使用范围
#### 特点
###### 优点
###### 缺点

- Boosting
- Bagging
- AdaBoost
- Blending
- GBM
- Random Forest

## 模型对比

#### scikit-cheet-sheat

![](Main-Model-Img/sklearn-cheat-sheet.png)

- `分类`：数据量 > 50 & 预测类 & 有属性
  - [SGD分类器](https://scikit-learn.org/stable/modules/sgd.html#classification)：数据量 < 100K
  - [核近似](https://scikit-learn.org/stable/modules/kernel_approximation.html)：SGD分类器无法工作
  - [线性SVC](https://scikit-learn.org/stable/modules/svm.html#classification)：数据量 > 100K
  - [朴素贝叶斯](https://scikit-learn.org/stable/modules/naive_bayes.html)：线性SVC无法工作 & 文本数据
  - [K临近分类](https://scikit-learn.org/stable/modules/neighbors.html)：线性SVC无法工作 & 非文本数据
  - [SVC](https://scikit-learn.org/stable/modules/svm.html#classification) and [集成分类器](https://scikit-learn.org/stable/modules/ensemble.html)：K临近分类无法工作
- `回归`：数据量 > 50 & 预测量
  - [SGD回归](https://scikit-learn.org/stable/modules/sgd.html#regression)：数据量 > 100K
  - [Lasso](https://scikit-learn.org/stable/modules/linear_model.html#lasso) and [ElasticNet](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net)：数据量 < 100K & 少量特征重要
  - [岭回归](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression) and [SVR（核=线性）](https://scikit-learn.org/stable/modules/svm.html#regression)：数据量 < 100K & 大量特征重要
  - [SVR（核=rbf）](https://scikit-learn.org/stable/modules/svm.html#regression) and [集成回归器](https://scikit-learn.org/stable/modules/ensemble.html)：岭回归 and SVR（核=线性） 无法工作
- `聚类`：数据量 > 50 & 预测类 & 无属性
  - [K均值](https://scikit-learn.org/stable/modules/clustering.html#k-means)：类别数量已知 & 数据量 < 10K
  - [光谱聚类](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering) and [GMM](https://scikit-learn.org/stable/modules/mixture.html)：K均值无法工作
  - [小批次-K均值](https://scikit-learn.org/stable/modules/clustering.html#mini-batch-k-means)类别数量已知 & 数据量 > 10K
  - [MeanShift](https://scikit-learn.org/stable/modules/clustering.html#mean-shift) and [VGBMM](https://scikit-learn.org/stable/modules/mixture.html#bgmm)：类别数量不明 & 数据量 < 10K
- `降维`：数据量 > 50 & 非预测类 & 非预测量 & 只是看看
  - [随机PCA](https://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca)：只是看看
  - [核近似](https://scikit-learn.org/stable/modules/kernel_approximation.html)：随机PCA无法工作 & 数据量 < 10K
  - [Isomap](https://scikit-learn.org/stable/modules/manifold.html#isomap) and [谱嵌入](https://scikit-learn.org/stable/modules/manifold.html#spectral-embedding)：随机PCA无法工作 & 数据量 > 10K
  - [LLE](https://scikit-learn.org/stable/modules/manifold.html#locally-linear-embedding)：Isomap and 谱嵌入 无法工作

### 主要算法对比

- GBDT 和随机森林区别（重点）
  - GBDT 和随机森林的相同点：
    - 都是由多棵树组成；
    - 最终的结果都由多棵树共同决定。
  - GBDT 和随机森林的不同点：
    - 组成随机森林的可以是分类树、回归树；组成 GBDT 只能是回归树；
    - 组成随机森林的树可以并行生成（Bagging）；GBDT 只能串行生成（Boosting）；这两种模型都用到了Bootstrap的思想。
    - 对于最终的输出结果而言，随机森林使用多数投票或者简单平均；而 GBDT 则是将所有结果累加起来，或者加权累加起来；
    - 随机森林对异常值不敏感，GBDT 对异常值非常敏感；
    - 随机森林对训练集一视同仁权值一样，GBDT 是基于权值的弱分类器的集成；
    - 随机森林通过减小模型的方差提高性能，GBDT 通过减少模型偏差提高性能。


# 链接

- [鱼遇雨欲语与余 | 武汉大学](https://www.zhihu.com/people/wang-he-13-93/activities)

- [常用的机器学习算法比较？ | 知乎](https://www.zhihu.com/question/27306416)
- https://www.zhihu.com/question/34470160/answer/114305935
- http://kubicode.me/2015/08/16/Machine%20Learning/Algorithm-Summary-for-Interview/
